{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68caed89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import hashlib\n",
    "import sqlite3\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from textblob import TextBlob\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb3f6771",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPipeline:\n",
    "    \"\"\"\n",
    "    Handles ingestion, normalization, and deduplication of customer support data\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, db_path: str = \"customer_support.db\"):\n",
    "        self.db_path = db_path\n",
    "        self.setup_database()\n",
    "    \n",
    "    def setup_database(self):\n",
    "        \"\"\"Create database tables with proper indexing\"\"\"\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Main interactions table\n",
    "        cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS interactions (\n",
    "                id TEXT PRIMARY KEY,\n",
    "                customer_id TEXT NOT NULL,\n",
    "                brand TEXT NOT NULL,\n",
    "                tweet_id TEXT UNIQUE,\n",
    "                author_id TEXT,\n",
    "                inbound BOOLEAN,\n",
    "                created_at TIMESTAMP,\n",
    "                text TEXT,\n",
    "                response_tweet_id TEXT,\n",
    "                in_response_to_tweet_id TEXT,\n",
    "                conversation_thread TEXT,\n",
    "                processed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "                FOREIGN KEY (conversation_thread) REFERENCES conversations(thread_id)\n",
    "            )\n",
    "        ''')\n",
    "        \n",
    "        # Conversation tracking table\n",
    "        cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS conversations (\n",
    "                thread_id TEXT PRIMARY KEY,\n",
    "                customer_id TEXT NOT NULL,\n",
    "                brand TEXT NOT NULL,\n",
    "                status TEXT DEFAULT 'open',\n",
    "                created_at TIMESTAMP,\n",
    "                last_updated TIMESTAMP,\n",
    "                total_interactions INTEGER DEFAULT 0,\n",
    "                customer_sentiment REAL,\n",
    "                issue_category TEXT,\n",
    "                resolution_status TEXT\n",
    "            )\n",
    "        ''')\n",
    "        \n",
    "        # Create indexes for performance\n",
    "        cursor.execute('CREATE INDEX IF NOT EXISTS idx_customer_id ON interactions(customer_id)')\n",
    "        cursor.execute('CREATE INDEX IF NOT EXISTS idx_thread ON interactions(conversation_thread)')\n",
    "        cursor.execute('CREATE INDEX IF NOT EXISTS idx_created_at ON interactions(created_at)')\n",
    "        \n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "    \n",
    "    def generate_interaction_id(self, record: Dict) -> str:\n",
    "        \"\"\"Generate unique, deterministic ID for deduplication\"\"\"\n",
    "        key_fields = f\"{record.get('tweet_id', '')}{record.get('author_id', '')}{record.get('created_at', '')}\"\n",
    "        return hashlib.md5(key_fields.encode()).hexdigest()\n",
    "    \n",
    "    def ingest_cst_data(self, df: pd.DataFrame) -> int:\n",
    "        \"\"\"\n",
    "        Ingest Customer Support Twitter data with idempotent processing\n",
    "        Returns number of new records processed\n",
    "        \"\"\"\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        new_records = 0\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            interaction_id = self.generate_interaction_id(row.to_dict())\n",
    "            \n",
    "            # Check if already exists\n",
    "            existing = pd.read_sql_query(\n",
    "                \"SELECT id FROM interactions WHERE id = ?\", \n",
    "                conn, params=[interaction_id]\n",
    "            )\n",
    "            \n",
    "            if len(existing) == 0:\n",
    "                # Determine conversation thread\n",
    "                thread_id = self._get_or_create_thread(row, conn)\n",
    "                \n",
    "                # Insert interaction\n",
    "                cursor = conn.cursor()\n",
    "                cursor.execute('''\n",
    "                    INSERT INTO interactions \n",
    "                    (id, customer_id, brand, tweet_id, author_id, inbound, \n",
    "                     created_at, text, response_tweet_id, in_response_to_tweet_id, \n",
    "                     conversation_thread)\n",
    "                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "                ''', (\n",
    "                    interaction_id,\n",
    "                    row.get('author_id', ''),\n",
    "                    row.get('brand', ''),\n",
    "                    row.get('tweet_id', ''),\n",
    "                    row.get('author_id', ''),\n",
    "                    row.get('inbound', True),\n",
    "                    row.get('created_at', ''),\n",
    "                    row.get('text', ''),\n",
    "                    row.get('response_tweet_id', ''),\n",
    "                    row.get('in_response_to_tweet_id', ''),\n",
    "                    thread_id\n",
    "                ))\n",
    "                new_records += 1\n",
    "        \n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "        return new_records\n",
    "    \n",
    "    def _get_or_create_thread(self, row: Dict, conn) -> str:\n",
    "        \"\"\"Get or create conversation thread ID\"\"\"\n",
    "        # Simple thread logic - could be enhanced with more sophisticated grouping\n",
    "        thread_key = f\"{row.get('author_id', '')}_{row.get('brand', '')}\"\n",
    "        thread_id = hashlib.md5(thread_key.encode()).hexdigest()[:16]\n",
    "        \n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute('SELECT thread_id FROM conversations WHERE thread_id = ?', [thread_id])\n",
    "        \n",
    "        if not cursor.fetchone():\n",
    "            cursor.execute('''\n",
    "                INSERT INTO conversations (thread_id, customer_id, brand, created_at, last_updated)\n",
    "                VALUES (?, ?, ?, ?, ?)\n",
    "            ''', (thread_id, row.get('author_id', ''), row.get('brand', ''), \n",
    "                  row.get('created_at', ''), row.get('created_at', '')))\n",
    "        \n",
    "        return thread_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b8898ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserBehaviorAnalyzer:\n",
    "    \"\"\"\n",
    "    Analyzes user behavior patterns and creates customer cohorts\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, db_path: str = \"customer_support.db\"):\n",
    "        self.db_path = db_path\n",
    "        self.customer_segments = {}\n",
    "        self.conversation_flows = {}\n",
    "    \n",
    "    def analyze_conversation_patterns(self) -> pd.DataFrame:\n",
    "        \"\"\"Analyze conversation flows and customer behavior\"\"\"\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        \n",
    "        # Get conversation data with interactions\n",
    "        query = '''\n",
    "            SELECT \n",
    "                c.thread_id,\n",
    "                c.customer_id,\n",
    "                c.brand,\n",
    "                COUNT(i.id) as interaction_count,\n",
    "                AVG(CASE WHEN i.inbound THEN 1 ELSE 0 END) as customer_msg_ratio,\n",
    "                GROUP_CONCAT(i.text, ' | ') as conversation_text,\n",
    "                c.created_at,\n",
    "                julianday('now') - julianday(c.last_updated) as days_since_last_update\n",
    "            FROM conversations c\n",
    "            LEFT JOIN interactions i ON c.thread_id = i.conversation_thread\n",
    "            GROUP BY c.thread_id\n",
    "            HAVING interaction_count > 0\n",
    "        '''\n",
    "        \n",
    "        df = pd.read_sql_query(query, conn)\n",
    "        conn.close()\n",
    "        \n",
    "        # Feature engineering\n",
    "        df['sentiment_score'] = df['conversation_text'].apply(self._get_sentiment)\n",
    "        df['urgency_keywords'] = df['conversation_text'].apply(self._count_urgency_keywords)\n",
    "        df['question_count'] = df['conversation_text'].apply(lambda x: x.count('?') if x else 0)\n",
    "        df['avg_message_length'] = df['conversation_text'].apply(\n",
    "            lambda x: np.mean([len(msg.strip()) for msg in x.split('|') if msg.strip()]) if x else 0\n",
    "        )\n",
    "        \n",
    "        # Determine issue categories\n",
    "        df['issue_category'] = df['conversation_text'].apply(self._categorize_issue)\n",
    "        \n",
    "        # Determine resolution status\n",
    "        df['is_resolved'] = df.apply(self._determine_resolution_status, axis=1)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def _get_sentiment(self, text: str) -> float:\n",
    "        \"\"\"Calculate sentiment score using TextBlob\"\"\"\n",
    "        if not text:\n",
    "            return 0.0\n",
    "        try:\n",
    "            return TextBlob(text).sentiment.polarity\n",
    "        except:\n",
    "            return 0.0\n",
    "    \n",
    "    def _count_urgency_keywords(self, text: str) -> int:\n",
    "        \"\"\"Count urgency indicators in text\"\"\"\n",
    "        if not text:\n",
    "            return 0\n",
    "        \n",
    "        urgency_words = [\n",
    "            'urgent', 'emergency', 'asap', 'immediately', 'critical', \n",
    "            'broken', 'not working', 'error', 'issue', 'problem',\n",
    "            'help', 'stuck', 'frustrated', 'angry'\n",
    "        ]\n",
    "        \n",
    "        text_lower = text.lower()\n",
    "        return sum(1 for word in urgency_words if word in text_lower)\n",
    "    \n",
    "    def _categorize_issue(self, text: str) -> str:\n",
    "        \"\"\"Categorize the nature of support request\"\"\"\n",
    "        if not text:\n",
    "            return 'unknown'\n",
    "        \n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        # Login/Authentication issues\n",
    "        if any(word in text_lower for word in ['login', 'password', 'account', 'authenticate', 'access']):\n",
    "            return 'authentication'\n",
    "        \n",
    "        # Billing issues\n",
    "        elif any(word in text_lower for word in ['bill', 'payment', 'charge', 'refund', 'subscription']):\n",
    "            return 'billing'\n",
    "        \n",
    "        # Technical issues\n",
    "        elif any(word in text_lower for word in ['bug', 'error', 'broken', 'crash', 'not working']):\n",
    "            return 'technical'\n",
    "        \n",
    "        # Product inquiries\n",
    "        elif any(word in text_lower for word in ['how to', 'feature', 'usage', 'tutorial']):\n",
    "            return 'product_inquiry'\n",
    "        \n",
    "        # Complaints\n",
    "        elif any(word in text_lower for word in ['complaint', 'dissatisfied', 'angry', 'terrible']):\n",
    "            return 'complaint'\n",
    "        \n",
    "        else:\n",
    "            return 'general'\n",
    "    \n",
    "    def _determine_resolution_status(self, row) -> bool:\n",
    "        \"\"\"Determine if issue appears resolved based on conversation patterns\"\"\"\n",
    "        text = row['conversation_text'] if row['conversation_text'] else ''\n",
    "        days_since_update = row['days_since_last_update']\n",
    "        \n",
    "        # Resolution indicators\n",
    "        resolution_phrases = [\n",
    "            'thank you', 'thanks', 'solved', 'resolved', 'fixed', \n",
    "            'working now', 'that helps', 'perfect', 'great'\n",
    "        ]\n",
    "        \n",
    "        # If contains resolution phrases and no recent activity\n",
    "        has_resolution_phrase = any(phrase in text.lower() for phrase in resolution_phrases)\n",
    "        no_recent_activity = days_since_update > 7\n",
    "        \n",
    "        return has_resolution_phrase or (no_recent_activity and row['interaction_count'] > 2)\n",
    "    \n",
    "    def create_customer_cohorts(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Create customer cohorts based on behavior patterns\"\"\"\n",
    "        \n",
    "        # Prepare features for clustering\n",
    "        feature_cols = ['interaction_count', 'sentiment_score', 'urgency_keywords', \n",
    "                       'question_count', 'avg_message_length', 'days_since_last_update']\n",
    "        \n",
    "        # Fill NaN values\n",
    "        df[feature_cols] = df[feature_cols].fillna(0)\n",
    "        \n",
    "        # Standardize features\n",
    "        scaler = StandardScaler()\n",
    "        scaled_features = scaler.fit_transform(df[feature_cols])\n",
    "        \n",
    "        # Perform clustering\n",
    "        kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "        df['customer_cohort'] = kmeans.fit_predict(scaled_features)\n",
    "        \n",
    "        # Name cohorts based on characteristics\n",
    "        cohort_names = {\n",
    "            0: 'quick_resolvers',\n",
    "            1: 'high_engagement',\n",
    "            2: 'frustrated_customers',\n",
    "            3: 'dormant_cases',\n",
    "            4: 'complex_issues'\n",
    "        }\n",
    "        \n",
    "        df['cohort_name'] = df['customer_cohort'].map(cohort_names)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def visualize_patterns(self, df: pd.DataFrame):\n",
    "        \"\"\"Create visualizations of user behavior patterns\"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        \n",
    "        # Sentiment distribution by cohort\n",
    "        sns.boxplot(data=df, x='cohort_name', y='sentiment_score', ax=axes[0,0])\n",
    "        axes[0,0].set_title('Sentiment Score by Customer Cohort')\n",
    "        axes[0,0].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Issue category distribution\n",
    "        issue_counts = df['issue_category'].value_counts()\n",
    "        axes[0,1].pie(issue_counts.values, labels=issue_counts.index, autopct='%1.1f%%')\n",
    "        axes[0,1].set_title('Distribution of Issue Categories')\n",
    "        \n",
    "        # Resolution status by cohort\n",
    "        resolution_df = df.groupby('cohort_name')['is_resolved'].mean().reset_index()\n",
    "        sns.barplot(data=resolution_df, x='cohort_name', y='is_resolved', ax=axes[1,0])\n",
    "        axes[1,0].set_title('Resolution Rate by Cohort')\n",
    "        axes[1,0].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Interaction count distribution\n",
    "        sns.histplot(data=df, x='interaction_count', bins=20, ax=axes[1,1])\n",
    "        axes[1,1].set_title('Distribution of Interaction Counts')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e213bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NextBestActionEngine:\n",
    "    \"\"\"\n",
    "    NBA engine that determines optimal next action for customer interactions\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, db_path: str = \"customer_support.db\"):\n",
    "        self.db_path = db_path\n",
    "        self.action_rules = self._define_action_rules()\n",
    "    \n",
    "    def _define_action_rules(self) -> Dict:\n",
    "        \"\"\"Define NBA rules based on customer segments and conversation context\"\"\"\n",
    "        return {\n",
    "            'quick_resolvers': {\n",
    "                'primary_channel': 'twitter_dm_reply',\n",
    "                'timing_delay_hours': 2,\n",
    "                'escalation_threshold': 2\n",
    "            },\n",
    "            'high_engagement': {\n",
    "                'primary_channel': 'twitter_dm_reply',\n",
    "                'timing_delay_hours': 1,\n",
    "                'escalation_threshold': 3\n",
    "            },\n",
    "            'frustrated_customers': {\n",
    "                'primary_channel': 'scheduling_phone_call',\n",
    "                'timing_delay_hours': 0.5,\n",
    "                'escalation_threshold': 1\n",
    "            },\n",
    "            'dormant_cases': {\n",
    "                'primary_channel': 'email_reply',\n",
    "                'timing_delay_hours': 24,\n",
    "                'escalation_threshold': 1\n",
    "            },\n",
    "            'complex_issues': {\n",
    "                'primary_channel': 'email_reply',\n",
    "                'timing_delay_hours': 4,\n",
    "                'escalation_threshold': 2\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def predict_next_action(self, customer_data: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        Predict next best action for a customer\n",
    "        \"\"\"\n",
    "        cohort = customer_data.get('cohort_name', 'quick_resolvers')\n",
    "        rules = self.action_rules.get(cohort, self.action_rules['quick_resolvers'])\n",
    "        \n",
    "        # Determine channel based on multiple factors\n",
    "        channel = self._select_channel(customer_data, rules)\n",
    "        \n",
    "        # Calculate optimal send time\n",
    "        send_time = self._calculate_send_time(customer_data, rules)\n",
    "        \n",
    "        # Generate personalized message\n",
    "        message = self._generate_message(customer_data, channel)\n",
    "        \n",
    "        # Generate reasoning\n",
    "        reasoning = self._generate_reasoning(customer_data, channel, cohort)\n",
    "        \n",
    "        return {\n",
    "            'customer_id': customer_data.get('customer_id', ''),\n",
    "            'channel': channel,\n",
    "            'send_time': send_time,\n",
    "            'message': message,\n",
    "            'reasoning': reasoning\n",
    "        }\n",
    "    \n",
    "    def _select_channel(self, customer_data: Dict, rules: Dict) -> str:\n",
    "        \"\"\"Select optimal communication channel\"\"\"\n",
    "        sentiment = customer_data.get('sentiment_score', 0)\n",
    "        urgency = customer_data.get('urgency_keywords', 0)\n",
    "        interaction_count = customer_data.get('interaction_count', 0)\n",
    "        issue_category = customer_data.get('issue_category', 'general')\n",
    "        \n",
    "        # High urgency or negative sentiment -> phone call\n",
    "        if urgency >= 3 or sentiment <= -0.5:\n",
    "            return 'scheduling_phone_call'\n",
    "        \n",
    "        # Complex billing/technical issues with multiple interactions -> email\n",
    "        elif issue_category in ['billing', 'technical'] and interaction_count >= 3:\n",
    "            return 'email_reply'\n",
    "        \n",
    "        # Default to Twitter for quick, public resolution\n",
    "        else:\n",
    "            return 'twitter_dm_reply'\n",
    "    \n",
    "    def _calculate_send_time(self, customer_data: Dict, rules: Dict) -> str:\n",
    "        \"\"\"Calculate optimal send time\"\"\"\n",
    "        delay_hours = rules['timing_delay_hours']\n",
    "        \n",
    "        # Adjust based on urgency\n",
    "        urgency = customer_data.get('urgency_keywords', 0)\n",
    "        if urgency >= 2:\n",
    "            delay_hours = min(delay_hours, 1)  # Cap at 1 hour for urgent issues\n",
    "        \n",
    "        send_time = datetime.now() + timedelta(hours=delay_hours)\n",
    "        return send_time.isoformat() + 'Z'\n",
    "    \n",
    "    def _generate_message(self, customer_data: Dict, channel: str) -> str:\n",
    "        \"\"\"Generate personalized message based on context\"\"\"\n",
    "        issue_category = customer_data.get('issue_category', 'general')\n",
    "        sentiment = customer_data.get('sentiment_score', 0)\n",
    "        \n",
    "        # Base templates by channel\n",
    "        if channel == 'scheduling_phone_call':\n",
    "            base = \"Hi! I'd like to schedule a call to personally help resolve your {issue_type} concern. \"\n",
    "        elif channel == 'email_reply':\n",
    "            base = \"Thank you for reaching out about your {issue_type} issue. I'm sending detailed information to help resolve this. \"\n",
    "        else:  # twitter_dm_reply\n",
    "            base = \"Hi! I'm here to help with your {issue_type} question. \"\n",
    "        \n",
    "        # Customize based on sentiment\n",
    "        if sentiment <= -0.3:\n",
    "            base += \"I understand your frustration and want to make this right. \"\n",
    "        elif sentiment >= 0.3:\n",
    "            base += \"I appreciate your patience as we work through this together. \"\n",
    "        \n",
    "        # Add specific guidance based on issue type\n",
    "        issue_guidance = {\n",
    "            'authentication': \"Let me help you regain access to your account safely.\",\n",
    "            'billing': \"I'll review your account and explain any charges or help with refunds.\",\n",
    "            'technical': \"Let me troubleshoot this technical issue step by step.\",\n",
    "            'product_inquiry': \"I'm happy to explain how our features work.\",\n",
    "            'complaint': \"Your feedback is valuable and I want to address your concerns directly.\",\n",
    "            'general': \"I'm here to assist with whatever you need.\"\n",
    "        }\n",
    "        \n",
    "        guidance = issue_guidance.get(issue_category, issue_guidance['general'])\n",
    "        \n",
    "        return base.format(issue_type=issue_category) + guidance\n",
    "    \n",
    "    def _generate_reasoning(self, customer_data: Dict, channel: str, cohort: str) -> str:\n",
    "        \"\"\"Generate explanation for the NBA decision\"\"\"\n",
    "        reasoning_parts = []\n",
    "        \n",
    "        # Channel reasoning\n",
    "        if channel == 'scheduling_phone_call':\n",
    "            reasoning_parts.append(\"Phone call selected due to high urgency/negative sentiment requiring personal touch\")\n",
    "        elif channel == 'email_reply':\n",
    "            reasoning_parts.append(\"Email selected for complex issue requiring detailed explanation\")\n",
    "        else:\n",
    "            reasoning_parts.append(\"Twitter DM selected for quick, public resolution\")\n",
    "        \n",
    "        # Cohort reasoning\n",
    "        reasoning_parts.append(f\"Customer belongs to '{cohort}' segment\")\n",
    "        \n",
    "        # Context factors\n",
    "        sentiment = customer_data.get('sentiment_score', 0)\n",
    "        if sentiment <= -0.3:\n",
    "            reasoning_parts.append(\"negative sentiment detected\")\n",
    "        \n",
    "        urgency = customer_data.get('urgency_keywords', 0)\n",
    "        if urgency >= 2:\n",
    "            reasoning_parts.append(\"high urgency indicators present\")\n",
    "        \n",
    "        return \"; \".join(reasoning_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "def29a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NBAEvaluator:\n",
    "    \"\"\"\n",
    "    Evaluates the NBA system performance and generates results\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, pipeline: DataPipeline, analyzer: UserBehaviorAnalyzer, engine: NextBestActionEngine):\n",
    "        self.pipeline = pipeline\n",
    "        self.analyzer = analyzer\n",
    "        self.engine = engine\n",
    "    \n",
    "    def run_end_to_end_evaluation(self, sample_size: int = 1000) -> pd.DataFrame:\n",
    "        \"\"\"Run complete evaluation pipeline\"\"\"\n",
    "        \n",
    "        # Step 1: Analyze customer behavior\n",
    "        print(\"Analyzing customer behavior patterns...\")\n",
    "        behavior_df = self.analyzer.analyze_conversation_patterns()\n",
    "        cohort_df = self.analyzer.create_customer_cohorts(behavior_df)\n",
    "        \n",
    "        # Step 2: Filter resolved cases\n",
    "        open_cases = cohort_df[~cohort_df['is_resolved']].copy()\n",
    "        resolved_count = len(cohort_df) - len(open_cases)\n",
    "        \n",
    "        print(f\"Total cases: {len(cohort_df)}\")\n",
    "        print(f\"Already resolved: {resolved_count}\")\n",
    "        print(f\"Open cases for NBA: {len(open_cases)}\")\n",
    "        \n",
    "        # Step 3: Sample for evaluation\n",
    "        if len(open_cases) > sample_size:\n",
    "            eval_cases = open_cases.sample(n=sample_size, random_state=42)\n",
    "        else:\n",
    "            eval_cases = open_cases.copy()\n",
    "        \n",
    "        # Step 4: Generate NBA predictions\n",
    "        print(f\"Generating NBA predictions for {len(eval_cases)} cases...\")\n",
    "        predictions = []\n",
    "        \n",
    "        for _, row in eval_cases.iterrows():\n",
    "            prediction = self.engine.predict_next_action(row.to_dict())\n",
    "            prediction.update({\n",
    "                'chat_log': self._format_chat_log(row),\n",
    "                'issue_status': self._predict_issue_status(row, prediction),\n",
    "                'current_sentiment': row['sentiment_score'],\n",
    "                'issue_category': row['issue_category'],\n",
    "                'cohort_name': row['cohort_name']\n",
    "            })\n",
    "            predictions.append(prediction)\n",
    "        \n",
    "        results_df = pd.DataFrame(predictions)\n",
    "        \n",
    "        # Step 5: Calculate success metrics\n",
    "        self._calculate_success_metrics(results_df)\n",
    "        \n",
    "        return results_df\n",
    "    \n",
    "    def _format_chat_log(self, row) -> str:\n",
    "        \"\"\"Format conversation history into readable chat log\"\"\"\n",
    "        conversation_text = row.get('conversation_text', '')\n",
    "        if not conversation_text:\n",
    "            return \"No conversation history available\"\n",
    "        \n",
    "        # Simple formatting - in reality would need more sophisticated parsing\n",
    "        messages = conversation_text.split(' | ')\n",
    "        formatted_log = []\n",
    "        \n",
    "        for i, msg in enumerate(messages[:10]):  # Limit to 10 messages\n",
    "            if msg.strip():\n",
    "                role = \"Customer\" if i % 2 == 0 else \"Support_agent\"\n",
    "                formatted_log.append(f\"{role}: {msg.strip()}\")\n",
    "        \n",
    "        return \"\\n\".join(formatted_log)\n",
    "    \n",
    "    def _predict_issue_status(self, row, prediction) -> str:\n",
    "        \"\"\"Predict likely issue status after action\"\"\"\n",
    "        channel = prediction['channel']\n",
    "        sentiment = row.get('sentiment_score', 0)\n",
    "        cohort = row.get('cohort_name', '')\n",
    "        issue_category = row.get('issue_category', '')\n",
    "        \n",
    "        # Complex logic for status prediction\n",
    "        if channel == 'scheduling_phone_call':\n",
    "            if sentiment <= -0.5:\n",
    "                return 'escalated'\n",
    "            else:\n",
    "                return 'resolved'\n",
    "        \n",
    "        elif channel == 'email_reply':\n",
    "            if issue_category in ['billing', 'technical']:\n",
    "                return 'pending_customer_reply'\n",
    "            else:\n",
    "                return 'resolved'\n",
    "        \n",
    "        else:  # twitter_dm_reply\n",
    "            if cohort == 'quick_resolvers':\n",
    "                return 'resolved'\n",
    "            else:\n",
    "                return 'pending_customer_reply'\n",
    "    \n",
    "    def _calculate_success_metrics(self, results_df: pd.DataFrame):\n",
    "        \"\"\"Calculate and display success metrics\"\"\"\n",
    "        total_cases = len(results_df)\n",
    "        \n",
    "        # Channel distribution\n",
    "        channel_dist = results_df['channel'].value_counts()\n",
    "        print(\"\\nChannel Distribution:\")\n",
    "        for channel, count in channel_dist.items():\n",
    "            print(f\"  {channel}: {count} ({count/total_cases*100:.1f}%)\")\n",
    "        \n",
    "        # Predicted resolution rates\n",
    "        resolution_dist = results_df['issue_status'].value_counts()\n",
    "        print(\"\\nPredicted Issue Status Distribution:\")\n",
    "        for status, count in resolution_dist.items():\n",
    "            print(f\"  {status}: {count} ({count/total_cases*100:.1f}%)\")\n",
    "        \n",
    "        # Success prediction by cohort\n",
    "        cohort_success = results_df.groupby('cohort_name')['issue_status'].apply(\n",
    "            lambda x: (x == 'resolved').sum() / len(x) * 100\n",
    "        ).round(1)\n",
    "        \n",
    "        print(\"\\nPredicted Resolution Rate by Cohort:\")\n",
    "        for cohort, rate in cohort_success.items():\n",
    "            print(f\"  {cohort}: {rate}%\")\n",
    "    \n",
    "    def export_results_csv(self, results_df: pd.DataFrame, filename: str = \"nba_results.csv\"):\n",
    "        \"\"\"Export results in the specified format\"\"\"\n",
    "        \n",
    "        # Select and order columns as specified\n",
    "        output_columns = [\n",
    "            'customer_id', 'channel', 'send_time', 'message', 'reasoning',\n",
    "            'chat_log', 'issue_status'\n",
    "        ]\n",
    "        \n",
    "        export_df = results_df[output_columns].copy()\n",
    "        export_df.to_csv(filename, index=False)\n",
    "        print(f\"\\nResults exported to {filename}\")\n",
    "        \n",
    "        return export_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef4386ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo usage and testing\n",
    "def create_sample_data():\n",
    "    \"\"\"Create sample data for demonstration\"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    sample_data = []\n",
    "    brands = ['Nike', 'Starbucks', 'Apple', 'Amazon', 'Tesla']\n",
    "    issue_types = ['login', 'billing', 'technical', 'product inquiry', 'complaint']\n",
    "    \n",
    "    for i in range(2000):\n",
    "        sample_data.append({\n",
    "            'tweet_id': f\"tweet_{i}\",\n",
    "            'author_id': f\"user_{i % 500}\",  # 500 unique users\n",
    "            'brand': np.random.choice(brands),\n",
    "            'inbound': np.random.choice([True, False], p=[0.6, 0.4]),\n",
    "            'created_at': f\"2024-{np.random.randint(1,13):02d}-{np.random.randint(1,29):02d}T{np.random.randint(0,24):02d}:00:00Z\",\n",
    "            'text': f\"Sample {np.random.choice(issue_types)} message {i}\",\n",
    "            'response_tweet_id': f\"response_{i}\" if np.random.random() > 0.5 else None,\n",
    "            'in_response_to_tweet_id': f\"tweet_{max(0, i-1)}\" if i > 0 and np.random.random() > 0.7 else None\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(sample_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de341655",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    print(\"=== Next-Best-Action System Demo ===\\n\")\n",
    "    \n",
    "    # Initialize components\n",
    "    pipeline = DataPipeline()\n",
    "    analyzer = UserBehaviorAnalyzer()\n",
    "    engine = NextBestActionEngine()\n",
    "    evaluator = NBAEvaluator(pipeline, analyzer, engine)\n",
    "    \n",
    "    # Create and ingest sample data\n",
    "    print(\"1. Creating sample data...\")\n",
    "    sample_df = create_sample_data()\n",
    "    new_records = pipeline.ingest_cst_data(sample_df)\n",
    "    print(f\"   Ingested {new_records} new records\")\n",
    "    \n",
    "    # Run end-to-end evaluation\n",
    "    print(\"\\n2. Running end-to-end evaluation...\")\n",
    "    results_df = evaluator.run_end_to_end_evaluation(sample_size=1000)\n",
    "    \n",
    "    # Export results\n",
    "    print(\"\\n3. Exporting results...\")\n",
    "    final_export = evaluator.export_results_csv(results_df)\n",
    "    \n",
    "    print(\"\\n=== Demo Complete ===\")\n",
    "    print(f\"Processed {len(results_df)} customer cases\")\n",
    "    print(\"Check 'nba_results.csv' for detailed results\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results = main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
